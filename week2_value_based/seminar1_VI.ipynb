{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov decision process\n",
    "\n",
    "This week's methods are all built to solve __M__arkov __D__ecision __P__rocesses. In the broadest sense, an MDP is defined by how it changes states and how rewards are computed.\n",
    "\n",
    "State transition is defined by $P(s' |s,a)$ - how likely are you to end at state $s'$ if you take action $a$ from state $s$. Now there's more than one way to define rewards, but we'll use $r(s,a,s')$ function for convenience.\n",
    "\n",
    "_This notebook is inspired by the awesome_ [CS294](https://github.com/berkeleydeeprlcourse/homework/tree/master/sp17_hw/hw2) _by Berkeley_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For starters, let's define a simple MDP from this picture:\n",
    "<img src='https://s17.postimg.org/mawroys8f/750px-_Markov_Decision_Process_example.png' width=300px>\n",
    "_img by MistWiz (Own work) [Public domain], via Wikimedia Commons_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probs = {\n",
    "  's0':{\n",
    "    'a0': {'s0': 0.5, 's2': 0.5},\n",
    "    'a1': {'s2': 1}\n",
    "  },\n",
    "  's1':{\n",
    "    'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "    'a1': {'s1': 0.95, 's2': 0.05}\n",
    "  },\n",
    "  's2':{\n",
    "    'a0': {'s0': 0.4, 's1': 0.6},\n",
    "    'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
    "  }\n",
    "}\n",
    "rewards = {\n",
    "  's1': {'a0': {'s0': +5}},\n",
    "  's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "\n",
    "from mdp import MDP\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use MDP just as any other gym environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial state = s0\n",
      "next_state = s2, reward = 0.0, done = False\n"
     ]
    }
   ],
   "source": [
    "print('initial state =', mdp.reset())\n",
    "next_state, reward, done, info = mdp.step('a1')\n",
    "print('next_state = %s, reward = %s, done = %s' % (next_state, reward, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but it also has other methods that you'll need for Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdp.get_all_states = ('s0', 's1', 's2')\n",
      "mdp.get_possible_actions('s1') =  ('a0', 'a1')\n",
      "mdp.get_next_states('s1', 'a0') =  {'s0': 0.7, 's1': 0.1, 's2': 0.2}\n",
      "mdp.get_reward('s1', 'a0', 's0') =  5\n",
      "mdp.get_transition_prob('s1', 'a0', 's0') =  0.7\n"
     ]
    }
   ],
   "source": [
    "print(\"mdp.get_all_states =\", mdp.get_all_states())\n",
    "print(\"mdp.get_possible_actions('s1') = \", mdp.get_possible_actions('s1'))\n",
    "print(\"mdp.get_next_states('s1', 'a0') = \", mdp.get_next_states('s1', 'a0'))\n",
    "print(\"mdp.get_reward('s1', 'a0', 's0') = \", mdp.get_reward('s1', 'a0', 's0'))\n",
    "print(\"mdp.get_transition_prob('s1', 'a0', 's0') = \", mdp.get_transition_prob('s1', 'a0', 's0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'s2': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp.get_next_states('s0', 'a1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "\n",
    "Now let's build something to solve this MDP. The simplest algorithm so far is __V__alue __I__teration\n",
    "\n",
    "Here's the pseudo-code for VI:\n",
    "\n",
    "---\n",
    "\n",
    "`1.` Initialize $V^{(0)}(s)=0$, for all $s$\n",
    "\n",
    "`2.` For $i=0, 1, 2, \\dots$\n",
    " \n",
    "`3.` $ \\quad V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$, for all $s$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's write a function to compute the state-action value function $Q^{\\pi}$, defined as follows\n",
    "\n",
    "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_value(mdp, state_values, state, action, gamma):\n",
    "    \"\"\" Computes Q(s,a) as in formula above \"\"\"\n",
    "    q = 0.0\n",
    "    for next_s in mdp.get_next_states(state, action):\n",
    "        trans_prob = mdp.get_transition_prob(state, action, next_s)\n",
    "        reward = mdp.get_reward(state, action, next_s)\n",
    "        \n",
    "        q += trans_prob * (reward + gamma * state_values[next_s])\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_action_value(mdp, test_Vs, 's2', 'a1', 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test_Vs = {s : i for i, s in enumerate(sorted(mdp.get_all_states()))}\n",
    "assert np.allclose(get_action_value(mdp, test_Vs, 's2', 'a1', 0.9), 0.69)\n",
    "assert np.allclose(get_action_value(mdp, test_Vs, 's1', 'a0', 0.9), 3.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $Q(s,a)$ we can now define the \"next\" V(s) for value iteration.\n",
    " $$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_state_value(mdp, state_values, state, gamma):\n",
    "    \"\"\" Computes next V(s) as in formula above. Please do not change state_values in process. \"\"\"\n",
    "    if mdp.is_terminal(state): return 0\n",
    "    action_values = []\n",
    "    for next_a in mdp.get_possible_actions(state):\n",
    "        action_value = get_action_value(mdp, state_values, \n",
    "                                        state, next_a, gamma\n",
    "                                       )\n",
    "        action_values.append(action_value)\n",
    "    \n",
    "    return  max(action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Vs_copy = dict(test_Vs)\n",
    "assert np.allclose(get_new_state_value(mdp, test_Vs, 's0', 0.9), 1.8)\n",
    "assert np.allclose(get_new_state_value(mdp, test_Vs, 's2', 0.9), 0.69)\n",
    "assert test_Vs == test_Vs_copy, \"please do not change state_values in get_new_state_value\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's combine everything we wrote into a working value iteration algo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    0   |   diff: 3.50000   |   V(s0) = 0.000   V(s1) = 0.000   V(s2) = 0.000\n",
      "\n",
      "iter    1   |   diff: 1.89000   |   V(s0) = 0.000   V(s1) = 3.500   V(s2) = 0.000\n",
      "\n",
      "iter    2   |   diff: 1.70100   |   V(s0) = 0.000   V(s1) = 3.815   V(s2) = 1.890\n",
      "\n",
      "iter    3   |   diff: 1.13542   |   V(s0) = 1.701   V(s1) = 4.184   V(s2) = 2.060\n",
      "\n",
      "iter    4   |   diff: 0.73024   |   V(s0) = 1.854   V(s1) = 5.319   V(s2) = 2.871\n",
      "\n",
      "iter    5   |   diff: 0.61135   |   V(s0) = 2.584   V(s1) = 5.664   V(s2) = 3.540\n",
      "\n",
      "iter    6   |   diff: 0.54664   |   V(s0) = 3.186   V(s1) = 6.275   V(s2) = 3.989\n",
      "\n",
      "iter    7   |   diff: 0.49198   |   V(s0) = 3.590   V(s1) = 6.790   V(s2) = 4.535\n",
      "\n",
      "iter    8   |   diff: 0.42210   |   V(s0) = 4.082   V(s1) = 7.189   V(s2) = 4.959\n",
      "\n",
      "iter    9   |   diff: 0.36513   |   V(s0) = 4.463   V(s1) = 7.611   V(s2) = 5.352\n",
      "\n",
      "iter   10   |   diff: 0.32862   |   V(s0) = 4.816   V(s1) = 7.960   V(s2) = 5.717\n",
      "\n",
      "iter   11   |   diff: 0.29262   |   V(s0) = 5.145   V(s1) = 8.280   V(s2) = 6.032\n",
      "\n",
      "iter   12   |   diff: 0.26189   |   V(s0) = 5.429   V(s1) = 8.572   V(s2) = 6.323\n",
      "\n",
      "iter   13   |   diff: 0.23503   |   V(s0) = 5.691   V(s1) = 8.830   V(s2) = 6.584\n",
      "\n",
      "iter   14   |   diff: 0.21124   |   V(s0) = 5.925   V(s1) = 9.065   V(s2) = 6.817\n",
      "\n",
      "iter   15   |   diff: 0.19012   |   V(s0) = 6.135   V(s1) = 9.276   V(s2) = 7.028\n",
      "\n",
      "iter   16   |   diff: 0.17091   |   V(s0) = 6.325   V(s1) = 9.465   V(s2) = 7.218\n",
      "\n",
      "iter   17   |   diff: 0.15366   |   V(s0) = 6.496   V(s1) = 9.636   V(s2) = 7.388\n",
      "\n",
      "iter   18   |   diff: 0.13830   |   V(s0) = 6.649   V(s1) = 9.790   V(s2) = 7.542\n",
      "\n",
      "iter   19   |   diff: 0.12445   |   V(s0) = 6.788   V(s1) = 9.928   V(s2) = 7.680\n",
      "\n",
      "iter   20   |   diff: 0.11200   |   V(s0) = 6.912   V(s1) = 10.052   V(s2) = 7.805\n",
      "\n",
      "iter   21   |   diff: 0.10079   |   V(s0) = 7.024   V(s1) = 10.164   V(s2) = 7.917\n",
      "\n",
      "iter   22   |   diff: 0.09071   |   V(s0) = 7.125   V(s1) = 10.265   V(s2) = 8.017\n",
      "\n",
      "iter   23   |   diff: 0.08164   |   V(s0) = 7.216   V(s1) = 10.356   V(s2) = 8.108\n",
      "\n",
      "iter   24   |   diff: 0.07347   |   V(s0) = 7.297   V(s1) = 10.437   V(s2) = 8.190\n",
      "\n",
      "iter   25   |   diff: 0.06612   |   V(s0) = 7.371   V(s1) = 10.511   V(s2) = 8.263\n",
      "\n",
      "iter   26   |   diff: 0.05951   |   V(s0) = 7.437   V(s1) = 10.577   V(s2) = 8.329\n",
      "\n",
      "iter   27   |   diff: 0.05356   |   V(s0) = 7.496   V(s1) = 10.636   V(s2) = 8.389\n",
      "\n",
      "iter   28   |   diff: 0.04820   |   V(s0) = 7.550   V(s1) = 10.690   V(s2) = 8.442\n",
      "\n",
      "iter   29   |   diff: 0.04338   |   V(s0) = 7.598   V(s1) = 10.738   V(s2) = 8.491\n",
      "\n",
      "iter   30   |   diff: 0.03904   |   V(s0) = 7.641   V(s1) = 10.782   V(s2) = 8.534\n",
      "\n",
      "iter   31   |   diff: 0.03514   |   V(s0) = 7.681   V(s1) = 10.821   V(s2) = 8.573\n",
      "\n",
      "iter   32   |   diff: 0.03163   |   V(s0) = 7.716   V(s1) = 10.856   V(s2) = 8.608\n",
      "\n",
      "iter   33   |   diff: 0.02846   |   V(s0) = 7.747   V(s1) = 10.887   V(s2) = 8.640\n",
      "\n",
      "iter   34   |   diff: 0.02562   |   V(s0) = 7.776   V(s1) = 10.916   V(s2) = 8.668\n",
      "\n",
      "iter   35   |   diff: 0.02306   |   V(s0) = 7.801   V(s1) = 10.941   V(s2) = 8.694\n",
      "\n",
      "iter   36   |   diff: 0.02075   |   V(s0) = 7.824   V(s1) = 10.964   V(s2) = 8.717\n",
      "\n",
      "iter   37   |   diff: 0.01867   |   V(s0) = 7.845   V(s1) = 10.985   V(s2) = 8.738\n",
      "\n",
      "iter   38   |   diff: 0.01681   |   V(s0) = 7.864   V(s1) = 11.004   V(s2) = 8.756\n",
      "\n",
      "iter   39   |   diff: 0.01513   |   V(s0) = 7.881   V(s1) = 11.021   V(s2) = 8.773\n",
      "\n",
      "iter   40   |   diff: 0.01361   |   V(s0) = 7.896   V(s1) = 11.036   V(s2) = 8.788\n",
      "\n",
      "iter   41   |   diff: 0.01225   |   V(s0) = 7.909   V(s1) = 11.049   V(s2) = 8.802\n",
      "\n",
      "iter   42   |   diff: 0.01103   |   V(s0) = 7.922   V(s1) = 11.062   V(s2) = 8.814\n",
      "\n",
      "iter   43   |   diff: 0.00992   |   V(s0) = 7.933   V(s1) = 11.073   V(s2) = 8.825\n",
      "\n",
      "iter   44   |   diff: 0.00893   |   V(s0) = 7.943   V(s1) = 11.083   V(s2) = 8.835\n",
      "\n",
      "iter   45   |   diff: 0.00804   |   V(s0) = 7.952   V(s1) = 11.092   V(s2) = 8.844\n",
      "\n",
      "iter   46   |   diff: 0.00724   |   V(s0) = 7.960   V(s1) = 11.100   V(s2) = 8.852\n",
      "\n",
      "iter   47   |   diff: 0.00651   |   V(s0) = 7.967   V(s1) = 11.107   V(s2) = 8.859\n",
      "\n",
      "iter   48   |   diff: 0.00586   |   V(s0) = 7.973   V(s1) = 11.113   V(s2) = 8.866\n",
      "\n",
      "iter   49   |   diff: 0.00527   |   V(s0) = 7.979   V(s1) = 11.119   V(s2) = 8.872\n",
      "\n",
      "iter   50   |   diff: 0.00475   |   V(s0) = 7.984   V(s1) = 11.125   V(s2) = 8.877\n",
      "\n",
      "iter   51   |   diff: 0.00427   |   V(s0) = 7.989   V(s1) = 11.129   V(s2) = 8.882\n",
      "\n",
      "iter   52   |   diff: 0.00384   |   V(s0) = 7.993   V(s1) = 11.134   V(s2) = 8.886\n",
      "\n",
      "iter   53   |   diff: 0.00346   |   V(s0) = 7.997   V(s1) = 11.137   V(s2) = 8.890\n",
      "\n",
      "iter   54   |   diff: 0.00311   |   V(s0) = 8.001   V(s1) = 11.141   V(s2) = 8.893\n",
      "\n",
      "iter   55   |   diff: 0.00280   |   V(s0) = 8.004   V(s1) = 11.144   V(s2) = 8.896\n",
      "\n",
      "iter   56   |   diff: 0.00252   |   V(s0) = 8.007   V(s1) = 11.147   V(s2) = 8.899\n",
      "\n",
      "iter   57   |   diff: 0.00227   |   V(s0) = 8.009   V(s1) = 11.149   V(s2) = 8.902\n",
      "\n",
      "iter   58   |   diff: 0.00204   |   V(s0) = 8.011   V(s1) = 11.152   V(s2) = 8.904\n",
      "\n",
      "iter   59   |   diff: 0.00184   |   V(s0) = 8.014   V(s1) = 11.154   V(s2) = 8.906\n",
      "\n",
      "iter   60   |   diff: 0.00166   |   V(s0) = 8.015   V(s1) = 11.155   V(s2) = 8.908\n",
      "\n",
      "iter   61   |   diff: 0.00149   |   V(s0) = 8.017   V(s1) = 11.157   V(s2) = 8.909\n",
      "\n",
      "iter   62   |   diff: 0.00134   |   V(s0) = 8.019   V(s1) = 11.159   V(s2) = 8.911\n",
      "\n",
      "iter   63   |   diff: 0.00121   |   V(s0) = 8.020   V(s1) = 11.160   V(s2) = 8.912\n",
      "\n",
      "iter   64   |   diff: 0.00109   |   V(s0) = 8.021   V(s1) = 11.161   V(s2) = 8.913\n",
      "\n",
      "iter   65   |   diff: 0.00098   |   V(s0) = 8.022   V(s1) = 11.162   V(s2) = 8.915\n",
      "\n",
      "Terminated\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "gamma = 0.9            # discount for MDP\n",
    "num_iter = 100         # maximum iterations, excluding initialization\n",
    "min_difference = 0.001 # stop VI if new values are this close to old values (or closer)\n",
    "\n",
    "# initialize V(s)\n",
    "state_values = {s : 0 for s in mdp.get_all_states()}\n",
    "\n",
    "\n",
    "for i in range(num_iter):\n",
    "    \n",
    "    # Compute new state values using the functions you defined above.\n",
    "    # It must be a dict {state : float V_new(state)}\n",
    "    new_state_values = {state: get_new_state_value(mdp, state_values, state, gamma) for state in mdp.get_all_states()}\n",
    "    \n",
    "    assert isinstance(new_state_values, dict)\n",
    "    \n",
    "    # Compute difference\n",
    "    diff = max(abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states())\n",
    "    print(\"iter %4i   |   diff: %6.5f   |   \"%(i, diff), end=\"\")\n",
    "    print('   '.join(\"V(%s) = %.3f\"%(s, v) for s,v in state_values.items()), end='\\n\\n')\n",
    "    state_values = new_state_values\n",
    "    \n",
    "    if diff < min_difference:\n",
    "        print(\"Terminated\"); break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final state values: {'s0': 8.023123818663871, 's1': 11.163174814980803, 's2': 8.915559364985523}\n"
     ]
    }
   ],
   "source": [
    "print(\"Final state values:\", state_values)\n",
    "\n",
    "assert abs(state_values['s0'] - 8.032)  < 0.01\n",
    "assert abs(state_values['s1'] - 11.169) < 0.01\n",
    "assert abs(state_values['s2'] - 8.921)  < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use those $V^{*}(s)$ to find optimal actions in each state\n",
    "\n",
    " $$\\pi^*(s) = argmax_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = argmax_a Q_i(s,a)$$\n",
    " \n",
    "The only difference vs V(s) is that here we take not max but argmax: find action such with maximum Q(s,a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_action(mdp, state_values, state, gamma=0.9):\n",
    "    \"\"\" Finds optimal action using formula above. \"\"\"\n",
    "    if mdp.is_terminal(state): return None\n",
    "    \n",
    "    next_a = mdp.get_possible_actions(state)\n",
    "    q_vals = [get_action_value(mdp, state_values, state, act, gamma) for act in next_a ]\n",
    "    #print('opt A', state, next_a, q_vals, state_values)\n",
    "    return next_a[np.argmax(q_vals)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_optimal_action(mdp, state_values, 's0', gamma) == 'a1'\n",
    "assert get_optimal_action(mdp, state_values, 's1', gamma) == 'a0'\n",
    "assert get_optimal_action(mdp, state_values, 's2', gamma) == 'a0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward:  0.912\n"
     ]
    }
   ],
   "source": [
    "# Measure agent's average reward\n",
    "\n",
    "s = mdp.reset()\n",
    "rewards = []\n",
    "for _ in range(10000):\n",
    "    s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
    "    rewards.append(r)\n",
    "    \n",
    "print(\"average reward: \", np.mean(rewards))\n",
    "\n",
    "assert(0.85 < np.mean(rewards) < 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frozen lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*FFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mdp import FrozenLakeEnv\n",
    "mdp = FrozenLakeEnv(slip_chance=0)\n",
    "\n",
    "mdp.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp, state_values=None, gamma = 0.9, num_iter = 1000, min_difference = 1e-5):\n",
    "    \"\"\" performs num_iter value iteration steps starting from state_values. Same as before but in a function \"\"\"\n",
    "    state_values = state_values or {s : 0 for s in mdp.get_all_states()}\n",
    "    for i in range(num_iter):\n",
    "\n",
    "        # Compute new state values using the functions you defined above. It must be a dict {state : new_V(state)}\n",
    "        new_state_values = {state: get_new_state_value(mdp, state_values, state, gamma) \n",
    "                            for state in mdp.get_all_states()}\n",
    "        \n",
    "        assert isinstance(new_state_values, dict)\n",
    "\n",
    "        # Compute difference\n",
    "        diff =  max(abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states())\n",
    "        \n",
    "        print(\"iter %4i   |   diff: %6.5f   |   V(start): %.3f \"%(i, diff, new_state_values[mdp._initial_state]))\n",
    "        \n",
    "        state_values = new_state_values\n",
    "        if diff < min_difference:\n",
    "            break\n",
    "            \n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_values = value_iteration(mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = mdp.reset()\n",
    "mdp.render()\n",
    "for t in range(100):\n",
    "    a = get_optimal_action(mdp, state_values, s, gamma)\n",
    "    print(a, end='\\n\\n')\n",
    "    s, r, done, _ = mdp.step(a)\n",
    "    mdp.render()\n",
    "    if done: break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize!\n",
    "\n",
    "It's usually interesting to see what your algorithm actually learned under the hood. To do so, we'll plot state value functions and optimal actions at each VI step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def draw_policy(mdp, state_values):\n",
    "    plt.figure(figsize=(3,3))\n",
    "    h,w = mdp.desc.shape\n",
    "    states = sorted(mdp.get_all_states())\n",
    "    V = np.array([state_values[s] for s in states])\n",
    "    Pi = {s: get_optimal_action(mdp, state_values, s, gamma) for s in states}\n",
    "    plt.imshow(V.reshape(w,h), cmap='gray', interpolation='none', clim=(0,1))\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(h)-.5)\n",
    "    ax.set_yticks(np.arange(w)-.5)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    Y, X = np.mgrid[0:4, 0:4]\n",
    "    a2uv = {'left': (-1, 0), 'down':(0, -1), 'right':(1,0), 'up':(-1, 0)}\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            plt.text(x, y, str(mdp.desc[y,x].item()),\n",
    "                     color='g', size=12,  verticalalignment='center',\n",
    "                     horizontalalignment='center', fontweight='bold')\n",
    "            a = Pi[y, x]\n",
    "            if a is None: continue\n",
    "            u, v = a2uv[a]\n",
    "            plt.arrow(x, y,u*.3, -v*.3, color='m', head_width=0.1, head_length=0.1) \n",
    "    plt.grid(color='b', lw=2, ls='-')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_values = {s : 0 for s in mdp.get_all_states()}\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"after iteration %i\"%i)\n",
    "    state_values = value_iteration(mdp, state_values, num_iter=1)\n",
    "    draw_policy(mdp, state_values)\n",
    "# please ignore iter 0 at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "mdp = FrozenLakeEnv(map_name='8x8',slip_chance=0.1)\n",
    "state_values = {s : 0 for s in mdp.get_all_states()}\n",
    "\n",
    "for i in range(30):\n",
    "    clear_output(True)\n",
    "    print(\"after iteration %i\"%i)\n",
    "    state_values = value_iteration(mdp, state_values, num_iter=1)\n",
    "    draw_policy(mdp, state_values)\n",
    "    sleep(0.5)\n",
    "# please ignore iter 0 at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Massive tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = FrozenLakeEnv(slip_chance=0)\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards.append(r)\n",
    "        if done: break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "    \n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(1.0 <= np.mean(total_rewards) <= 1.0)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure agent's average reward\n",
    "mdp = FrozenLakeEnv(slip_chance=0.1)\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards.append(r)\n",
    "        if done: break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "    \n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(0.8 <= np.mean(total_rewards) <= 0.95)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure agent's average reward\n",
    "mdp = FrozenLakeEnv(slip_chance=0.25)\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards.append(r)\n",
    "        if done: break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "    \n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(0.6 <= np.mean(total_rewards) <= 0.7)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure agent's average reward\n",
    "mdp = FrozenLakeEnv(slip_chance=0.2, map_name='8x8')\n",
    "state_values = value_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards.append(r)\n",
    "        if done: break\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "    \n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(0.6 <= np.mean(total_rewards) <= 0.8)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My conclusion\n",
    "1. Three days and bonus 1 is done! I don't know why but if mdp have two diferent action from one state to anyother - reward became first action will be 0!!! It fucking sheet!!  \n",
    "'s2': {'a0': {'s2': r0}},\n",
    "'s0': {'a1': {'s0': 5}},\n",
    "'s0': {'a0': {'s0':88}}\n",
    "That mean mdp.get_reward('s0', 'a1', 's0') get 0 instead of 5 \n",
    "It's need create one more states and it became not triveial as at graph lower..  or maybe I don't know math.\n",
    "\n",
    "Interesting fact for my mdp. What is best policy for my mdp? a0 or a1? What does it mean this iteration and convergense. if I play long game with 100 steps action a1 get ~12 and af*mdp + a1 get 50 \n",
    "#for 100 step a1/a0 = 11 / 50\n",
    "#for 10 steps a1/a0 = 11 / 7\n",
    "#for ~20 steps they equal\n",
    "\n",
    "So question \"for how many steps it's best policy\"?? \n",
    "(and I have some bag, yerstarday change strategy was with r0 = 0.73 (at 51 step) but today r0 = 0.74 looks like yerstaday it did not do discount (do not know why))\n",
    "\n",
    "\n",
    "2. Bonus two - very tired with states without action (singular matrix or 0 at answer on frozen lake)... Help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus 1 - find an MDP for which value iteration takes long to converge  (2+ pts)\n",
    "\n",
    "When we ran value iteration on the small frozen lake problem, the last iteration where an action changed was iteration 6--i.e., value iteration computed the optimal policy at iteration 6. Are there any guarantees regarding how many iterations it'll take value iteration to compute the optimal policy? There are no such guarantees without additional assumptions--we can construct the MDP in such a way that the greedy policy will change after arbitrarily many iterations.\n",
    "\n",
    "Your task: define an MDP with at most 3 states and 2 actions, such that when you run value iteration, the optimal action changes at iteration >= 50. Use discount=0.95. (However, note that the discount doesn't matter here--you can construct an appropriate MDP with any discount.)\n",
    "\n",
    "Note: value function must change at least once after iteration >=50, not necessarily change on every iteration till >=50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denis/anaconda3/envs/tourch_gym/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NotImplemented"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def noteq(a, b):\n",
    "    return np.array([a[i] != b[i] for i in range(len(a))])\n",
    "np.not_equal(['a1', 'a0'], ['a1', 'a0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "History = defaultdict(list)\n",
    "#print(History[2])\n",
    "\n",
    "def get_action_value(mdp, state_values, state, action, gamma):\n",
    "    \"\"\" Computes Q(s,a) as in formula above \"\"\"\n",
    "    q = 0.0\n",
    "    for next_s in mdp.get_next_states(state, action):\n",
    "        trans_prob = mdp.get_transition_prob(state, action, next_s)\n",
    "        reward = mdp.get_reward(state, action, next_s)\n",
    "        \n",
    "        q += trans_prob * (reward + gamma * state_values[next_s])\n",
    "        #print('action', action, trans_prob, reward, 'stv', next_s, state_values[next_s], q)\n",
    "        \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_state_value(mdp, state_values, state, gamma):\n",
    "    \"\"\" Computes next V(s) as in formula above. Please do not change state_values in process. \"\"\"\n",
    "    \n",
    "    if mdp.is_terminal(state): return 0\n",
    "    action_values = []\n",
    "    for next_a in mdp.get_possible_actions(state):\n",
    "        action_value = get_action_value(mdp, state_values, \n",
    "                                        state, next_a, gamma\n",
    "                                       )\n",
    "        action_values.append(action_value)\n",
    "        History[next_a].append(action_value)\n",
    "    #print('ss', action_values, state_values )\n",
    "    return  max(action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp, state_values=None, gamma = 0.9, num_iter = 1000, min_difference = 1e-5):\n",
    "    \"\"\" performs num_iter value iteration steps starting from state_values. Same as before but in a function \"\"\"\n",
    "    state_values = state_values or {s : 0 for s in mdp.get_all_states()}\n",
    "    for i in range(num_iter):\n",
    "\n",
    "        # Compute new state values using the functions you defined above. It must be a dict {state : new_V(state)}\n",
    "        new_state_values = {state: get_new_state_value(mdp, state_values, state, gamma) \n",
    "                            for state in mdp.get_all_states()}\n",
    "        \n",
    "        assert isinstance(new_state_values, dict)\n",
    "\n",
    "        # Compute difference\n",
    "        diff =  max(abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states())\n",
    "        \n",
    "        print(\"iter %4i   |   diff: %6.5f   |   V(start): %.3f \"%(i, diff, new_state_values[mdp._initial_state]))\n",
    "        \n",
    "        state_values = new_state_values\n",
    "        if diff < min_difference:\n",
    "            break\n",
    "            \n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# That main Idea\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "a1 = 0.1\n",
    "r1 = 10\n",
    "a = [0,]\n",
    "def av(p,r,a):\n",
    "    for i in range(60):\n",
    "        a.append(p * (r + 0.95 * a[-1]))\n",
    "    return a\n",
    "\n",
    "plt.plot(range(61), av(a1, r1, a), 'r')\n",
    "#print(a)\n",
    "a0 = 0.9\n",
    "r0 = 2\n",
    "b = [0,]\n",
    "plt.plot(range(61), av(a0, r0, b))\n",
    "#print(b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0 = 0.99\n",
    "a1 = 0.1\n",
    "r0 = 0.74 # or 0.73\n",
    "r1 = 100\n",
    "\n",
    "transition_probs = {\n",
    "  's0':{ \n",
    "    'a1': {'s0': a1, 's1': 1 - a1},\n",
    "    'afuckingmdp': {'s2': 1}\n",
    "    \n",
    "  },\n",
    "  's1':{\n",
    "    'a':{'s1':1}\n",
    "   },\n",
    "   's2':{\n",
    "    'a0':{'s2':a0, 's1':1 - a0}\n",
    "   } \n",
    "}\n",
    "    \n",
    "rewards = {\n",
    " \n",
    "  's2': {'a0': {'s2': r0}},\n",
    "  's0': {'a1': {'s0': r1}},\n",
    "  #'s0': {'a0': {'s0':88}}\n",
    "  #'s1': {'a': {'s0': -0.4}} \n",
    "}\n",
    "\n",
    "from mdp import MDP\n",
    "mdp = MDP(transition_probs, rewards)\n",
    "\n",
    "mdp._initial_state = 's0'#('s0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    0   |   diff: 10.00000   |   V(start): 10.000 \n",
      "iter    0   |   diff: 0.95000   |   V(start): 10.950 \n",
      "iter    0   |   diff: 0.64801   |   V(start): 11.040 \n",
      "iter    0   |   diff: 0.60946   |   V(start): 11.049 \n",
      "iter    0   |   diff: 0.57319   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.53909   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.50701   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.47685   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.44847   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.42179   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.39669   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.37309   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.35089   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.33001   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.31038   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.29191   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.27454   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.25821   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.24284   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.22839   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.21480   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.20202   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.19000   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.17870   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.16807   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.15807   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.14866   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.13982   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.13150   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.12367   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.11631   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.10939   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.10288   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.09676   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.09101   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.08559   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.08050   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.07571   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.07120   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.06697   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.06298   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.05923   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.05571   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.05240   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.04928   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.04635   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.04359   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.04099   |   V(start): 11.050 \n",
      "iter    0   |   diff: 0.03856   |   V(start): 11.081 \n",
      "iter    0   |   diff: 0.03663   |   V(start): 11.118 \n",
      "iter    0   |   diff: 0.03445   |   V(start): 11.152 \n",
      "iter    0   |   diff: 0.03240   |   V(start): 11.185 \n",
      "iter    0   |   diff: 0.03047   |   V(start): 11.215 \n",
      "iter    0   |   diff: 0.02866   |   V(start): 11.244 \n",
      "iter    0   |   diff: 0.02695   |   V(start): 11.271 \n",
      "iter    0   |   diff: 0.02535   |   V(start): 11.296 \n",
      "iter    0   |   diff: 0.02384   |   V(start): 11.320 \n",
      "iter    0   |   diff: 0.02242   |   V(start): 11.343 \n",
      "iter    0   |   diff: 0.02109   |   V(start): 11.364 \n",
      "iter    0   |   diff: 0.01983   |   V(start): 11.383 \n",
      "iter    0   |   diff: 0.01865   |   V(start): 11.402 \n",
      "iter    0   |   diff: 0.01754   |   V(start): 11.420 \n",
      "iter    0   |   diff: 0.01650   |   V(start): 11.436 \n",
      "iter    0   |   diff: 0.01552   |   V(start): 11.452 \n",
      "iter    0   |   diff: 0.01459   |   V(start): 11.466 \n",
      "iter    0   |   diff: 0.01373   |   V(start): 11.480 \n",
      "iter    0   |   diff: 0.01291   |   V(start): 11.493 \n",
      "iter    0   |   diff: 0.01214   |   V(start): 11.505 \n",
      "iter    0   |   diff: 0.01142   |   V(start): 11.516 \n",
      "iter    0   |   diff: 0.01074   |   V(start): 11.527 \n",
      "iter    0   |   diff: 0.01010   |   V(start): 11.537 \n",
      "iter    0   |   diff: 0.00950   |   V(start): 11.547 \n",
      "iter    0   |   diff: 0.00893   |   V(start): 11.556 \n",
      "iter    0   |   diff: 0.00840   |   V(start): 11.564 \n",
      "iter    0   |   diff: 0.00790   |   V(start): 11.572 \n",
      "iter    0   |   diff: 0.00743   |   V(start): 11.579 \n",
      "iter    0   |   diff: 0.00699   |   V(start): 11.586 \n",
      "iter    0   |   diff: 0.00657   |   V(start): 11.593 \n",
      "iter    0   |   diff: 0.00618   |   V(start): 11.599 \n",
      "iter    0   |   diff: 0.00582   |   V(start): 11.605 \n",
      "iter    0   |   diff: 0.00547   |   V(start): 11.611 \n",
      "iter    0   |   diff: 0.00514   |   V(start): 11.616 \n",
      "iter    0   |   diff: 0.00484   |   V(start): 11.621 \n",
      "iter    0   |   diff: 0.00455   |   V(start): 11.625 \n",
      "iter    0   |   diff: 0.00428   |   V(start): 11.629 \n",
      "iter    0   |   diff: 0.00402   |   V(start): 11.633 \n",
      "iter    0   |   diff: 0.00379   |   V(start): 11.637 \n",
      "iter    0   |   diff: 0.00356   |   V(start): 11.641 \n",
      "iter    0   |   diff: 0.00335   |   V(start): 11.644 \n",
      "iter    0   |   diff: 0.00315   |   V(start): 11.647 \n",
      "iter    0   |   diff: 0.00296   |   V(start): 11.650 \n",
      "iter    0   |   diff: 0.00279   |   V(start): 11.653 \n",
      "iter    0   |   diff: 0.00262   |   V(start): 11.656 \n",
      "iter    0   |   diff: 0.00246   |   V(start): 11.658 \n",
      "iter    0   |   diff: 0.00232   |   V(start): 11.660 \n",
      "iter    0   |   diff: 0.00218   |   V(start): 11.663 \n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-4916df4d1fa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#n_changes = np.not_equal(policy, new_policy).sum()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mn_changes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoteq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn_changes\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;31m#print(\"N actions changed = %i \\n\" % n_changes, policy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "History.clear()\n",
    "state_values = {s : 0 for s in mdp.get_all_states()}\n",
    "policy = [get_optimal_action(mdp, state_values, state, gamma) \n",
    "          for state in sorted(mdp.get_all_states())]\n",
    "\n",
    "for i in range(100): #100\n",
    "    #print(\"after iteration %i\"%i)\n",
    "    state_values = value_iteration(mdp, state_values, num_iter=1, gamma=0.95)\n",
    "    \n",
    "    #print('-->',state_values, policy)\n",
    "    new_policy = [get_optimal_action(mdp, state_values, state, gamma) \n",
    "                  for state in sorted(mdp.get_all_states())]\n",
    "    \n",
    "    #n_changes = np.not_equal(policy, new_policy).sum()\n",
    "    n_changes = noteq(policy, new_policy).sum()\n",
    "    assert not n_changes or i < 50\n",
    "    #print(\"N actions changed = %i \\n\" % n_changes, policy)\n",
    "    policy = new_policy\n",
    "    \n",
    "# please ignore iter 0 at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "afuckingmdp_disc = 0.95 * np.array(History['afuckingmdp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(History['a0'], 'r')\n",
    "plt.plot(History['a1'], 'b')\n",
    "plt.plot(History['afuckingmdp'], 'g')\n",
    "plt.plot(afuckingmdp_disc , 'g^')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s0', ('s2', 0.0, False, {}), ('s2', 0.74, False, {}))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp.reset(), mdp.step('afuckingmdp'), mdp.step('a0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wich policy is realy beter? Expirement\n",
    "# for 100 step a1/a0 = 11 / 50\n",
    "#for 10 steps a1/a0 = 11 / 7\n",
    "# for ~20 steps they equal\n",
    "\n",
    "my_policy = ['a1', 'a']#['afuckingmdp', 'a0', 'a'] # ['a1', 'a']\n",
    "med_rew = []\n",
    "for j in range(1000):\n",
    "    r = 0\n",
    "    mdp.reset()\n",
    "    for i in range(20):\n",
    "        for a in my_policy:\n",
    "            try:\n",
    "                r += mdp.step(a)[1]\n",
    "            except:\n",
    "                pass\n",
    "    med_rew.append(r)\n",
    "sum(med_rew) /  1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus 2 - Policy Iteration (3+ points)\n",
    "\n",
    "Let's implement exact policy iteration (PI), which has the following pseudocode:\n",
    "\n",
    "---\n",
    "Initialize $\\pi_0$   `// random or fixed action`\n",
    "\n",
    "For $n=0, 1, 2, \\dots$\n",
    "- Compute the state-value function $V^{\\pi_{n}}$\n",
    "- Using $V^{\\pi_{n}}$, compute the state-action-value function $Q^{\\pi_{n}}$\n",
    "- Compute new policy $\\pi_{n+1}(s) = \\operatorname*{argmax}_a Q^{\\pi_{n}}(s,a)$\n",
    "---\n",
    "\n",
    "Unlike VI, policy iteration has to maintain a policy - chosen actions from all states - and estimate $V^{\\pi_{n}}$ based on this policy. It only changes policy once values converged.\n",
    "\n",
    "\n",
    "Below are a few helpers that you may or may not use in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probs = {\n",
    "  's0':{\n",
    "    'a0': {'s0': 0.5, 's2': 0.5},\n",
    "    'a1': {'s2': 1}\n",
    "  },\n",
    "  's1':{\n",
    "    'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "    'a1': {'s1': 0.95, 's2': 0.05}\n",
    "  },\n",
    "  's2':{\n",
    "    'a0': {'s0': 0.4, 's1': 0.6},\n",
    "    'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
    "  }\n",
    "}\n",
    "rewards = {\n",
    "  's1': {'a0': {'s0': +5}},\n",
    "  's2': {'a1': {'s0': -1}}\n",
    "}\n",
    "\n",
    "from mdp import MDP\n",
    "mdp = MDP(transition_probs, rewards, initial_state='s0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function called `compute_vpi` that computes the state-value function $V^{\\pi}$ for an arbitrary policy $\\pi$.\n",
    "\n",
    "Unlike VI, this time you must find the exact solution, not just a single iteration.\n",
    "\n",
    "Recall that $V^{\\pi}$ satisfies the following linear equation:\n",
    "$$V^{\\pi}(s) = \\sum_{s'} P(s,\\pi(s),s')[ R(s,\\pi(s),s') + \\gamma V^{\\pi}(s')]$$\n",
    "\n",
    "You'll have to solve a linear system in your code. (Find an exact solution, e.g., with `np.linalg.solve`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main problem in frozen Lake is singular matrix (or maybe any other bugs) and it still give me 0 reward. What do with states without action - any random add break all results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = FrozenLakeEnv(slip_chance=0.2, map_name='4x4')#, map_name='8x8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('left', 'down', 'right', 'up') (0, 0) False\n",
      "('left', 'down', 'right', 'up') (0, 1) False\n",
      "('left', 'down', 'right', 'up') (0, 2) False\n",
      "('left', 'down', 'right', 'up') (0, 3) False\n",
      "('left', 'down', 'right', 'up') (1, 0) False\n",
      "() (1, 1) True\n",
      "('left', 'down', 'right', 'up') (1, 2) False\n",
      "() (1, 3) True\n",
      "('left', 'down', 'right', 'up') (2, 0) False\n",
      "('left', 'down', 'right', 'up') (2, 1) False\n",
      "('left', 'down', 'right', 'up') (2, 2) False\n",
      "() (2, 3) True\n",
      "() (3, 0) True\n",
      "('left', 'down', 'right', 'up') (3, 1) False\n",
      "('left', 'down', 'right', 'up') (3, 2) False\n",
      "() (3, 3) True\n"
     ]
    }
   ],
   "source": [
    "legal_st_act = {} # st, [act, ..]\n",
    "\n",
    "legal_st = []\n",
    "for st in mdp.get_all_states():\n",
    "    if mdp.get_possible_actions(st) != ():\n",
    "        legal_st.append(st)\n",
    "    \n",
    "        \n",
    "    \n",
    "    print(mdp.get_possible_actions(st), st, mdp.is_terminal(st))\n",
    "    #for a in mdp.get_possible_actions(st):\n",
    "    #    if \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import matrix_rank\n",
    "\n",
    "\n",
    "def compute_vpi(mdp, policy, gamma):\n",
    "    \"\"\"\n",
    "    Computes V^pi(s) FOR ALL STATES under given policy.\n",
    "    :param policy: a dict of currently chosen actions {s : a}\n",
    "    :returns: a dict {state : V^pi(state) for all states}\n",
    "    \"\"\"\n",
    "    all_states = [st for st in mdp.get_all_states() if not mdp.is_terminal(st)]\n",
    "    l = len(all_states)\n",
    "    b = np.zeros([l])\n",
    "    A = np.zeros([l, l])\n",
    "    B = np.zeros([l, l])\n",
    "    for i, s in enumerate(all_states):\n",
    "        A[i,i] = 1#np.random.rand()\n",
    "        \n",
    "        for ns in mdp.get_next_states(s, policy[s]):\n",
    "            #print(ns, s, policy[s])\n",
    "            \n",
    "                \n",
    "            b[i] += mdp.get_transition_prob(s, policy[s], ns) * mdp.get_reward(s, policy[s], ns)\n",
    "            if ns in all_states:\n",
    "                A[i, all_states.index(ns)] -= gamma * mdp.get_transition_prob(s, policy[s], ns)\n",
    "                #B[i, all_states.index(ns)] = -gamma * mdp.get_transition_prob(s, policy[s], ns)\n",
    "  \n",
    "    #A -= np.identity(l)\n",
    "    #print(A)\n",
    "    #print(B)\n",
    "    #return\n",
    "    vs_x = {s: v for s, v in zip(all_states, np.linalg.solve(A,b))}\n",
    "    return vs_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#state_values, policy = policy_iteration(mdp)#, num_iter=10)\n",
    "#state_values, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#legal_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_value(mdp, state_values, state, action, gamma):\n",
    "    \"\"\" Computes Q(s,a) as in formula above \"\"\"\n",
    "    q = 0.0\n",
    "    for next_s in mdp.get_next_states(state, action):\n",
    "        trans_prob = mdp.get_transition_prob(state, action, next_s)\n",
    "        reward = mdp.get_reward(state, action, next_s)\n",
    "        try:\n",
    "            q += trans_prob * (reward + gamma * state_values[next_s])\n",
    "        except KeyError:\n",
    "            q += trans_prob * reward\n",
    "        #print('action', action, trans_prob, reward, 'stv', next_s, state_values[next_s], q)\n",
    "        \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_action(mdp, state_values, state, gamma=0.9):\n",
    "    \"\"\" Finds optimal action using formula above. \"\"\"\n",
    "    if mdp.is_terminal(state): return None\n",
    "    \n",
    "    next_a = mdp.get_possible_actions(state)\n",
    "    q_vals = [get_action_value(mdp, state_values, state, act, gamma) for act in next_a ]\n",
    "    #print('opt A', state, next_a, q_vals, state_values)\n",
    "    return next_a[np.argmax(q_vals)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've got new state values, it's time to update our policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_new_policy(mdp, vpi, gamma):\n",
    "    \"\"\"\n",
    "    Computes new policy as argmax of state values\n",
    "    :param vpi: a dict {state : V^pi(state) for all states}\n",
    "    :returns: a dict {state : optimal action for all states}\n",
    "    \"\"\"\n",
    "    new_policy = {}\n",
    "    for s in legal_st:#mdp.get_all_states():\n",
    "        # if mdp.get_possible_actions(s) != ():\n",
    "        act = get_optimal_action(mdp, vpi, s, gamma)\n",
    "        if act is not None:\n",
    "            new_policy[s] = act\n",
    "            \n",
    "                \n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0,\n",
       " {'s0': 0.0, 's1': 0.0, 's2': -0.7499999999999999},\n",
       " {'s0': 1, 's1': 0.0, 's2': -0.7499999999999999})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'s0': 0.0, 's1': 0.0, 's2': -0.7499999999999999}\n",
    "def dict_del(d1, d2):\n",
    "    s = 0\n",
    "    for a, b in zip(d1, d2):\n",
    "        s += np.abs(d1[a] - d2[b])\n",
    "    return s\n",
    "d2 = d.copy()\n",
    "d2['s0'] = 1\n",
    "dict_del(d, d2), d, d2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Main loop__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(mdp, policy=None, gamma = 0.9, num_iter = 1000, min_difference = 1e-5):\n",
    "    \"\"\" \n",
    "    Run the policy iteration loop for num_iter iterations or till difference between V(s) is below min_difference.\n",
    "    If policy is not given, initialize it at random.\n",
    "    \"\"\"\n",
    "    if policy is None:\n",
    "        policy = {}\n",
    "        for s in mdp.get_all_states():\n",
    "            if mdp.get_possible_actions(s) != (): \n",
    "                policy[s] = np.random.choice(mdp.get_possible_actions(s))\n",
    "            \n",
    "                \n",
    "    #print(policy)\n",
    "    old_vpi = compute_vpi(mdp, policy, gamma)\n",
    "    #print(policy, old_vpi)\n",
    "    for i in range(num_iter):\n",
    "        new_policy = compute_new_policy(mdp, old_vpi, gamma)\n",
    "        vpi = compute_vpi(mdp, new_policy, gamma)\n",
    "        \n",
    "        #print(old_vpi, new_policy)\n",
    "        if dict_del(old_vpi, vpi) < min_difference:\n",
    "            print(\"No diff\")\n",
    "            break\n",
    "        old_vpi = vpi.copy()\n",
    "    \n",
    "    return vpi, new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your PI Results__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before it not Frozen Lake all looks Ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure agent's average reward\n",
    "\n",
    "#s = mdp.reset()\n",
    "#rewards = []\n",
    "#for _ in range(10000):\n",
    "#    #print(get_optimal_action(mdp, state_values, s, gamma))\n",
    "#    s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s)) #gamma = 0.02\n",
    "#    rewards.append(r)\n",
    "#    \n",
    "#print(\"average reward: \", np.mean(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mdp.get_all_states(), mdp.get_next_states, mdp.get_possible_actions((1,1))\n",
    "#policy, state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No diff\n",
      "average reward:  0.734\n",
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "# Measure agent's average reward\n",
    "mdp = FrozenLakeEnv(slip_chance=0.2, map_name='4x4')#, map_name='8x8')\n",
    "state_values, policy = policy_iteration(mdp)\n",
    "\n",
    "total_rewards = []\n",
    "for game_i in range(1000):\n",
    "    s = mdp.reset()\n",
    "    rewards = []\n",
    "    for t in range(100):\n",
    "        s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
    "        rewards.append(r)\n",
    "        \n",
    "        if done: break\n",
    "    # print('r', rewards)\n",
    "    total_rewards.append(np.sum(rewards))\n",
    "    \n",
    "print(\"average reward: \", np.mean(total_rewards))\n",
    "assert(0.6 <= np.mean(total_rewards) <= 0.8)\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
