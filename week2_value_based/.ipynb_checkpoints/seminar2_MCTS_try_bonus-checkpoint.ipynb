{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "### The problem with MsPacman is reward is always zero with just MCTS in other seminar... \n",
    "- We have 9 actions for pacman and our planing go wide, for planing in 1000 iteration we go deeper just for 11 tree levels (steps), but the reward if we play just with random action began after ~ 200 steps... \n",
    "\n",
    "### Lets solve it by add agent to our planing tree which span tree only with \"holy\" 3 actions (not 9).\n",
    "- agent predict Qvalues (we choose 3 best) . If I correct Qvalues should be equal value_sum after rollout. \n",
    "- we do rollout (whith agent best action [not random] ) from each of three new leaves and compare value_sum after rollout with predicted qvalue - that how we get loss for update agent weights  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seminar: Monte-carlo tree search\n",
    "\n",
    "In this seminar, we'll implement a vanilla MCTS planning and use it to solve some Gym envs.\n",
    "\n",
    "But before we do that, we first need to modify gym env to allow saving and loading game states to facilitate backtracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.core import Wrapper\n",
    "from pickle import dumps,loads\n",
    "from collections import namedtuple\n",
    "\n",
    "#a container for get_result function below. Works just like tuple, but prettier\n",
    "ActionResult = namedtuple(\"action_result\",(\"snapshot\",\"observation\",\"reward\",\"is_done\",\"info\"))\n",
    "\n",
    "\n",
    "class WithSnapshots(Wrapper):\n",
    "    \"\"\"\n",
    "    Creates a wrapper that supports saving and loading environemnt states.\n",
    "    Required for planning algorithms.\n",
    "\n",
    "    This class will have access to the core environment as self.env, e.g.:\n",
    "    - self.env.reset()           #reset original env\n",
    "    - self.env.ale.cloneState()  #make snapshot for atari. load with .restoreState()\n",
    "    - ...\n",
    "\n",
    "    You can also use reset, step and render directly for convenience.\n",
    "    - s, r, done, _ = self.step(action)   #step, same as self.env.step(action)\n",
    "    - self.render(close=True)             #close window, same as self.env.render(close=True)\n",
    "    \"\"\"\n",
    "\n",
    "        \n",
    "    def get_snapshot(self):\n",
    "        \"\"\"\n",
    "        :returns: environment state that can be loaded with load_snapshot \n",
    "        Snapshots guarantee same env behaviour each time they are loaded.\n",
    "        \n",
    "        Warning! Snapshots can be arbitrary things (strings, integers, json, tuples)\n",
    "        Don't count on them being pickle strings when implementing MCTS.\n",
    "        \n",
    "        Developer Note: Make sure the object you return will not be affected by \n",
    "        anything that happens to the environment after it's saved.\n",
    "        You shouldn't, for example, return self.env. \n",
    "        In case of doubt, use pickle.dumps or deepcopy.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.render() #close popup windows since we can't pickle them\n",
    "        if self.unwrapped.viewer is not None:\n",
    "            self.unwrapped.viewer.close()\n",
    "            self.unwrapped.viewer = None\n",
    "        return dumps(self.env)\n",
    "    \n",
    "    def load_snapshot(self,snapshot):\n",
    "        \"\"\"\n",
    "        Loads snapshot as current env state.\n",
    "        Should not change snapshot inplace (in case of doubt, deepcopy).\n",
    "        \"\"\"\n",
    "        \n",
    "        assert not hasattr(self,\"_monitor\") or hasattr(self.env,\"_monitor\"), \"can't backtrack while recording\"\n",
    "\n",
    "        #self.render(close=True) #close popup windows since we can't load into them\n",
    "        self.env.close()\n",
    "        self.env = loads(snapshot)\n",
    "    \n",
    "    def get_result(self,snapshot,action):\n",
    "        \"\"\"\n",
    "        A convenience function that \n",
    "        - loads snapshot, \n",
    "        - commits action via self.step,\n",
    "        - and takes snapshot again :)\n",
    "        \n",
    "        :returns: next snapshot, next_observation, reward, is_done, info\n",
    "        \n",
    "        Basically it returns next snapshot and everything that env.step would have returned.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.load_snapshot(snapshot)\n",
    "        \n",
    "        #print('Wraper', action)\n",
    "        #print('Wraper', self.env.step(action))\n",
    "        next_observation, reward, is_done, info = self.env.step(action)\n",
    "        next_snapshot = self.get_snapshot()\n",
    "        \n",
    "        return ActionResult(next_snapshot,    #fill in the variables\n",
    "                            next_observation, \n",
    "                            reward, is_done, info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try out snapshots:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: <class '__main__.WithSnapshots'> doesn't implement 'reset' method, which is required for wrappers derived directly from Wrapper. Deprecated default implementation is used.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denis/anaconda3/envs/tourch_gym/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = WithSnapshots(gym.make(\"MsPacman-ramDeterministic-v0\"))\n",
    "env.reset()\n",
    "\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0, 112, 114, 115,  48,   3,  88,  88,  88,  88,  88,   0,  80,\n",
       "         80,  80,  50,  98,   0,   0,   3,   0,   0,   1,   0,   0,   1,\n",
       "          6,   6, 198,   4,  67,   0,  45,   1,   0, 198, 198,   0,   0,\n",
       "          1,   0,  32,  52,   0,   0, 120,   0, 100, 130,   0,   0, 112,\n",
       "          1, 222,   0,   0,   3,   0,   6,  80, 255, 255,   0, 255, 255,\n",
       "         80, 255, 255,  80, 255, 255,  80, 255, 255,  80, 191, 191,  80,\n",
       "        191, 191,  80, 191, 191,  80, 255, 255,  80, 255, 255,  80, 255,\n",
       "        255,  80, 255, 255,   0, 255, 255,  80, 255, 255,  20, 223,  43,\n",
       "        217, 123, 217, 123, 217, 123, 217, 123, 217, 123, 217, 221,   0,\n",
       "         63,   0,   0,   0,   0,   0,   2,  66, 240, 146, 215], dtype=uint8),\n",
       " 0.0,\n",
       " False,\n",
       " {'ale.lives': 3})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(env.action_space.sample()) # some times it was load cart Pole... bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(env.render('rgb_array'))\n",
    "\n",
    "#create first snapshot\n",
    "snap0 = env.get_snapshot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions, epsilon=0):\n",
    "        \"\"\"A simple DQN agent\"\"\"\n",
    "        nn.Module.__init__(self)\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        #img_c, img_w, img_h = state_shape\n",
    "        \n",
    "        # Define your network body here. Please make sure agent is fully contained here\n",
    "        \n",
    "       \n",
    "        \n",
    "        self.net = nn.Sequential(#nn.Conv1d(1,16,kernel_size=3, stride=2),\n",
    "                                 #nn.ReLU(),\n",
    "                                 #nn.Conv2d(16,32,kernel_size=3, stride=2),\n",
    "                                 #nn.ReLU(),\n",
    "                                 #nn.Conv2d(32,64,kernel_size=5, stride=3),\n",
    "                                 #nn.BatchNorm2d(64),\n",
    "                                 #nn.ReLU(),\n",
    "                                \n",
    "                                 #Flatten(),\n",
    "                                 \n",
    "                                 #nn.Linear(128, 256),\n",
    "                                 #nn.ReLU(),\n",
    "                                 nn.Linear(128, self.n_actions),\n",
    "                                 nn.ReLU(),\n",
    "                                )\n",
    "        \n",
    "            \n",
    "\n",
    "    def forward(self, state_t):\n",
    "        \"\"\"\n",
    "        takes agent's observation (Variable), returns qvalues (Variable)\n",
    "        :param state_t: a batch of 4-frame buffers, shape = [batch_size, 4, h, w]\n",
    "        Hint: if you're running on GPU, use state_t.cuda() right here.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Use your network to compute qvalues for given state\n",
    "        qvalues = self.net.forward(state_t)\n",
    "        #print(qvalues.shape, state_t.shape)\n",
    "        assert isinstance(qvalues, Variable) and qvalues.requires_grad, \"qvalues must be a torch variable with grad\"\n",
    "        assert len(qvalues.shape) == 2 and qvalues.shape[0] == state_t.shape[0] and qvalues.shape[1] == n_actions\n",
    "        \n",
    "        return qvalues\n",
    "    \n",
    "    def get_qvalues(self, states):\n",
    "        \"\"\"\n",
    "        like forward, but works on numpy arrays, not Variables\n",
    "        \"\"\"\n",
    "        states = Variable(torch.FloatTensor(np.asarray(states))).cuda()\n",
    "        qvalues = self.forward(states)\n",
    "        return qvalues.data.cpu().numpy()\n",
    "    \n",
    "    def sample_actions(self, qvalues):\n",
    "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "        \n",
    "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "        \n",
    "        #should_explore = np.random.choice([0, 1], batch_size, p = [1-epsilon, epsilon])\n",
    "        return best_actions#np.where(should_explore, random_actions, best_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = env.step(1)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQNAgent(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=9, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dim\n",
    "agent = DQNAgent(state_dim, n_actions, epsilon=0.9) # 0.9 not explore\n",
    "s = env.reset()\n",
    "#agent.get_qvalues()\n",
    "agent.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_env, act = env, 1\n",
    "q = agent.get_qvalues([env.step(1)[0],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS: Monte-Carlo tree search\n",
    "\n",
    "In this section, we'll implement the vanilla MCTS algorithm with UCB1-based node selection.\n",
    "\n",
    "We will start by implementing the `Node` class - a simple class that acts like MCTS node and supports some of the MCTS algorithm steps.\n",
    "\n",
    "This MCTS implementation makes some assumptions about the environment, you can find those _in the notes section at the end of the notebook_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(env,WithSnapshots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\" a tree node for MCTS \"\"\"\n",
    "    \n",
    "    #metadata:\n",
    "    parent = None          #parent Node\n",
    "    value_sum = 0.         #sum of state values from all visits (numerator)\n",
    "    times_visited = 0      #counter of visits (denominator)\n",
    "\n",
    "    \n",
    "    def __init__(self,parent,action):\n",
    "        \"\"\"\n",
    "        Creates and empty node with no children.\n",
    "        Does so by commiting an action and recording outcome.\n",
    "        \n",
    "        :param parent: parent Node\n",
    "        :param action: action to commit from parent Node\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.parent = parent\n",
    "        self.action = action        \n",
    "        self.children = set()       #set of child nodes\n",
    "\n",
    "        #get action outcome and save it\n",
    "        res = env.get_result(parent.snapshot,action)\n",
    "        self.snapshot,self.observation,self.immediate_reward,self.is_done,_ = res\n",
    "        \n",
    "        \n",
    "    def is_leaf(self):\n",
    "        return len(self.children)==0\n",
    "    \n",
    "    def is_root(self):\n",
    "        return self.parent is None\n",
    "    \n",
    "    def get_mean_value(self):\n",
    "        return self.value_sum / self.times_visited if self.times_visited !=0 else 0\n",
    "    \n",
    "    def ucb_score(self,scale=10,max_value=1e100):\n",
    "        \"\"\"\n",
    "        Computes ucb1 upper bound using current value and visit counts for node and it's parent.\n",
    "        \n",
    "        :param scale: Multiplies upper bound by that. From hoeffding inequality, assumes reward range to be [0,scale].\n",
    "        :param max_value: a value that represents infinity (for unvisited nodes)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if self.times_visited == 0:\n",
    "            return max_value\n",
    "        \n",
    "        #compute ucb-1 additive component (to be added to mean value)\n",
    "        #hint: you can use self.parent.times_visited for N times node was considered,\n",
    "        # and self.times_visited for n times it was visited\n",
    "        \n",
    "        U = np.sqrt(2 * np.log(self.parent.times_visited) / self.times_visited)\n",
    "        \n",
    "        return self.get_mean_value() + scale*U\n",
    "    \n",
    "    \n",
    "    #MCTS steps\n",
    "    \n",
    "    def select_best_leaf(self):\n",
    "        \"\"\"\n",
    "        Picks the leaf with highest priority to expand\n",
    "        Does so by recursively picking nodes with best UCB-1 score until it reaches the leaf.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self.is_leaf():\n",
    "            return self\n",
    "        \n",
    "        children = self.children\n",
    "        \n",
    "        # add random?? or desision\n",
    "        best_child = max(children, key=lambda x: x.ucb_score())\n",
    "        \n",
    "        return best_child.select_best_leaf()\n",
    "    \n",
    "    def expand(self, agent=agent):\n",
    "        \"\"\"\n",
    "        Expands the current node by creating all possible child nodes.\n",
    "        Then returns one of those children.\n",
    "        \"\"\"\n",
    "        \n",
    "        assert not self.is_done, \"can't expand from terminal state\"\n",
    "\n",
    "        for action in self.get_actions(agent=agent):\n",
    "            self.children.add(Node(self, action[0]))\n",
    "        \n",
    "        return self.select_best_leaf()\n",
    "    \n",
    "    def rollout(self,t_max=10**3, gamma=0.99, agent=agent):#**4\n",
    "        \"\"\"\n",
    "        Play the game from this state to the end (done) or for t_max steps.\n",
    "        \n",
    "        On each step, pick action at random (hint: env.action_space.sample()).\n",
    "        \n",
    "        Compute sum of rewards from current state till \n",
    "        Note 1: use env.action_space.sample() for random action\n",
    "        Note 2: if node is terminal (self.is_done is True), just return 0\n",
    "        \n",
    "        \"\"\"\n",
    "            \n",
    "        #set env into the appropriate state\n",
    "        env.load_snapshot(self.snapshot)\n",
    "        obs = self.observation\n",
    "        is_done = self.is_done\n",
    "        \n",
    "        # add clever desision, add children\n",
    "        \n",
    "        step = 0\n",
    "        \n",
    "        \n",
    "        rollout_reward = 0\n",
    "         \n",
    "        while not is_done and step < t_max and gamma ** step > 1e-12:\n",
    "            step += 1\n",
    "            act, _ = self.get_actions(obs, agent=agent)[0] # best action\n",
    "            obs, reward, is_done, info = env.step(act)\n",
    "            rollout_reward += (gamma ** step) * reward\n",
    "        \n",
    "\n",
    "        return rollout_reward\n",
    "    \n",
    "    def get_actions(self, obs=None, agent=agent):\n",
    "        \"\"\"\n",
    "        ADD MY FUNKTION\n",
    "          We add this function to predict action in rollout and expand with any outside agent\n",
    "        \"\"\"\n",
    "        if obs is None:\n",
    "            obs = self.observation\n",
    "        q_vals = agent.get_qvalues([obs,])\n",
    "        holy_3 = sorted([(i,v) for i, v  in enumerate(q_vals[0])], key=lambda x: x[1], reverse=True)[:3]\n",
    "        self.holy_3 = holy_3 # for agent train\n",
    "        return holy_3\n",
    "    \n",
    "    def propagate(self,child_value):\n",
    "        \"\"\"\n",
    "        Uses child value (sum of rewards) to update parents recursively.\n",
    "        \"\"\"\n",
    "        #compute node value\n",
    "        my_value = self.immediate_reward + child_value\n",
    "        \n",
    "        #update value_sum and times_visited\n",
    "        self.value_sum+=my_value\n",
    "        self.times_visited+=1\n",
    "        \n",
    "        #propagate upwards\n",
    "        if not self.is_root():\n",
    "            self.parent.propagate(my_value)\n",
    "        \n",
    "    def safe_delete(self):\n",
    "        \"\"\"safe delete to prevent memory leak in some python versions\"\"\"\n",
    "        del self.parent\n",
    "        for child in self.children:\n",
    "            child.safe_delete()\n",
    "            del child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Root(Node):\n",
    "    def __init__(self,snapshot,observation):\n",
    "        \"\"\"\n",
    "        creates special node that acts like tree root\n",
    "        :snapshot: snapshot (from env.get_snapshot) to start planning from\n",
    "        :observation: last environment observation\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.parent = self.action = None\n",
    "        self.children = set()       #set of child nodes\n",
    "        \n",
    "        \n",
    "        #root: load snapshot and observation\n",
    "        self.snapshot = snapshot\n",
    "        self.observation = observation\n",
    "        self.immediate_reward = 0\n",
    "        self.is_done=False\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_node(node):\n",
    "        \"\"\"initializes node as root\"\"\"\n",
    "        root = Root(node.snapshot,node.observation)\n",
    "        #copy data\n",
    "        copied_fields = [\"value_sum\",\"times_visited\",\"children\",\"is_done\"]\n",
    "        for field in copied_fields:\n",
    "            setattr(root,field,getattr(node,field))\n",
    "        return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_observation = env.reset()\n",
    "root_snapshot = env.get_snapshot()\n",
    "root = Root(root_snapshot,root_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(agent.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_agent(agent=agent, root=root):\n",
    "    #loss = 0\n",
    "    #print('cv0', root.value_sum)\n",
    "    #print_tree(root.children)\n",
    "         \n",
    "    if root.parent is not None:\n",
    "        node = root.parent\n",
    "        obs = torch.FloatTensor([node.observation,]).cuda()\n",
    "        #obs = obs / 255\n",
    "        qvals = agent.forward(obs)[0]\n",
    "        print(qvals)\n",
    "        print(node.holy_3, agent.get_qvalues([node.observation,]))\n",
    "        loss = 0\n",
    "        for child in node.children:\n",
    "            loss += (qvals[child.action] - child.value_sum)**2\n",
    "            #if agent.forward(obs)[0][child.action] == 0:\n",
    "            #    print('OBS', obs)\n",
    "            print('loss = ', loss, child.value_sum, child.action)\n",
    "\n",
    "            \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main MCTS loop\n",
    "\n",
    "With all we implemented, MCTS boils down to a trivial piece of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_mcts(root,n_iters=10):\n",
    "    \"\"\"\n",
    "    builds tree with monte-carlo tree search for n_iters iterations\n",
    "    :param root: tree node to plan from\n",
    "    :param n_iters: how many select-expand-simulate-propagete loops to make\n",
    "    \"\"\"\n",
    "    for _ in range(n_iters):\n",
    "\n",
    "        node = root.select_best_leaf()\n",
    "\n",
    "        if node.is_done:\n",
    "            node.propagate(0)\n",
    "\n",
    "        else: #node is not terminal\n",
    "            \n",
    "            #env.reset()\n",
    "            #self.env.close()\n",
    "            #self.render()\n",
    "            node.expand(agent=agent)\n",
    "            value = node.rollout(t_max=500)\n",
    "            #print('rol ', value)\n",
    "            node.propagate(value)\n",
    "            train_agent(agent, node)\n",
    "            print('n vs', node.value_sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan and execute\n",
    "In this section, we use the MCTS implementation to find optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n vs 40.422843006255114\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  4.2278,  0.0000,  0.1542,  0.0000,\n",
      "        19.5754], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "[(4, 54.81739), (0, 0.0), (1, 0.0)] [[ 0.          0.          0.          0.          4.2278185   0.\n",
      "   0.15423742  0.         19.575369  ]]\n",
      "loss =  tensor(1310.0800, device='cuda:0', grad_fn=<AddBackward0>) 40.422843006255114 4\n",
      "loss =  tensor(1310.1038, device='cuda:0', grad_fn=<AddBackward0>) 0.0 6\n",
      "loss =  tensor(1693.2988, device='cuda:0', grad_fn=<AddBackward0>) 0.0 8\n",
      "n vs 40.422843006255114\n",
      "tensor([     0.0000,      0.0000,      0.0000,      0.0000, 192983.6719,\n",
      "             0.0000,      0.0000,      0.0000,      0.0000], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "[(4, 54.81739), (0, 0.0), (1, 0.0)] [[     0.        0.        0.        0.   192983.67      0.        0.\n",
      "       0.        0.  ]]\n",
      "loss =  tensor(3.7227e+10, device='cuda:0', grad_fn=<AddBackward0>) 40.422843006255114 4\n",
      "loss =  tensor(3.7227e+10, device='cuda:0', grad_fn=<AddBackward0>) 32.08313727689611 6\n",
      "loss =  tensor(3.7227e+10, device='cuda:0', grad_fn=<AddBackward0>) 0.0 8\n",
      "n vs 32.08313727689611\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "[(4, 54.81739), (0, 0.0), (1, 0.0)] [[0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "loss =  tensor(1634.0063, device='cuda:0', grad_fn=<AddBackward0>) 40.422843006255114 4\n",
      "loss =  tensor(2663.3340, device='cuda:0', grad_fn=<AddBackward0>) 32.08313727689611 6\n",
      "loss =  tensor(3442.0847, device='cuda:0', grad_fn=<AddBackward0>) 27.90610656135666 8\n",
      "n vs 27.90610656135666\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "[(4, 44.186947), (6, 22.991392), (8, 0.86335105)] [[0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "loss =  tensor(778.7508, device='cuda:0', grad_fn=<AddBackward0>) 27.90610656135666 6\n",
      "loss =  tensor(778.7508, device='cuda:0', grad_fn=<AddBackward0>) 0.0 4\n",
      "loss =  tensor(778.7508, device='cuda:0', grad_fn=<AddBackward0>) 0.0 8\n",
      "n vs 27.90610656135666\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "[(4, 184663.84), (0, 0.0), (1, 0.0)] [[0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "loss =  tensor(778.7508, device='cuda:0', grad_fn=<AddBackward0>) 27.90610656135666 4\n",
      "loss =  tensor(778.7508, device='cuda:0', grad_fn=<AddBackward0>) 0.0 0\n",
      "loss =  tensor(778.7508, device='cuda:0', grad_fn=<AddBackward0>) 0.0 1\n",
      "n vs 27.90610656135666\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "[(4, 44.186947), (6, 22.991392), (8, 0.86335105)] [[0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "loss =  tensor(778.7508, device='cuda:0', grad_fn=<AddBackward0>) 27.90610656135666 6\n",
      "loss =  tensor(1557.5016, device='cuda:0', grad_fn=<AddBackward0>) 27.90610656135666 4\n",
      "loss =  tensor(1557.5016, device='cuda:0', grad_fn=<AddBackward0>) 0.0 8\n",
      "n vs 27.90610656135666\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "[(0, 0.0), (1, 0.0), (2, 0.0)] [[0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "loss =  tensor(778.7508, device='cuda:0', grad_fn=<AddBackward0>) 27.90610656135666 0\n",
      "loss =  tensor(778.7508, device='cuda:0', grad_fn=<AddBackward0>) 0.0 1\n",
      "loss =  tensor(778.7508, device='cuda:0', grad_fn=<AddBackward0>) 0.0 2\n",
      "n vs 27.90610656135666\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-47da0f7d3536>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#plan from root:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplan_mcts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-115-633257fd2772>\u001b[0m in \u001b[0;36mplan_mcts\u001b[0;34m(root, n_iters)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m#self.env.close()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m#self.render()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m#print('rol ', value)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-110-d7f07ab37dcf>\u001b[0m in \u001b[0;36mexpand\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_best_leaf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-110-d7f07ab37dcf>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parent, action)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#get action outcome and save it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnapshot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnapshot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimmediate_reward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_done\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-2f28f9bea90f>\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self, snapshot, action)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \"\"\"\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_snapshot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msnapshot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m#print('Wraper', action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-2f28f9bea90f>\u001b[0m in \u001b[0;36mload_snapshot\u001b[0;34m(self, snapshot)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m#self.render(close=True) #close popup windows since we can't load into them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msnapshot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msnapshot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tourch_gym/lib/python3.7/site-packages/gym/utils/ezpickle.py\u001b[0m in \u001b[0;36m__setstate__\u001b[0;34m(self, d)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"_ezpickle_args\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ezpickle_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_ezpickle_kwargs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ezpickle_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_ezpickle_args\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_ezpickle_kwargs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tourch_gym/lib/python3.7/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, game, obs_type, frameskip, repeat_action_probability)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetFloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'repeat_action_probability'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeat_action_probability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_action_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetMinimalActionSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tourch_gym/lib/python3.7/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mseed\u001b[0;34m(self, seed)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# Empirically, we need to seed before loading the ROM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetInt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'random_seed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadROM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mseed1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tourch_gym/lib/python3.7/site-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mloadROM\u001b[0;34m(self, rom_file)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloadROM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrom_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadROM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_as_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrom_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#plan from root:\n",
    "plan_mcts(root,n_iters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "(5, 0.0, 678.8622347254104, 42.17878033120482), (8, 0.0, 451.72209783184167, 42.54454303455728), (7, 0.0, 678.8622347254104, 42.17878033120482), \n",
      "\n",
      "(7, 0.0, 323.38954872425734, 44.02117556142233), (3, 0.0, 160.41568638448055, 42.93567426907714), (5, 0.0, 128.33254910758444, 44.21664249832005), (3, 0.0, 128.33254910758444, 43.40777713311568), (5, 0.0, 128.33254910758444, 43.40777713311568), (7, 0.0, 128.33254910758444, 43.40777713311568), (5, 0.0, 160.41568638448055, 42.93567426907714), (7, 0.0, 323.38954872425734, 44.02117556142233), (3, 0.0, 128.33254910758444, 44.21664249832005), \n",
      "\n",
      "(7, 0.0, 96.24941183068833, 44.186097183013345), (5, 0.0, 96.24941183068833, 44.186097183013345), (3, 0.0, 64.16627455379222, 46.906175350571225), (7, 0.0, 64.16627455379222, 44.769499688691305), (3, 0.0, 32.08313727689611, 50.024363056837124), (5, 0.0, 32.08313727689611, 50.024363056837124), (3, 0.0, 32.08313727689611, 48.73422950005006), (7, 0.0, 32.08313727689611, 48.73422950005006), (5, 0.0, 32.08313727689611, 48.73422950005006), (3, 0.0, 32.08313727689611, 48.73422950005006), (5, 0.0, 32.08313727689611, 48.73422950005006), (7, 0.0, 32.08313727689611, 48.73422950005006), (3, 0.0, 32.08313727689611, 48.73422950005006), (5, 0.0, 32.08313727689611, 48.73422950005006), (7, 0.0, 32.08313727689611, 48.73422950005006), (3, 0.0, 32.08313727689611, 48.73422950005006), (7, 0.0, 32.08313727689611, 48.73422950005006), (5, 0.0, 32.08313727689611, 48.73422950005006), (5, 0.0, 64.16627455379222, 44.769499688691305), (3, 0.0, 32.08313727689611, 50.024363056837124), (7, 0.0, 32.08313727689611, 50.024363056837124), (3, 0.0, 96.24941183068833, 44.186097183013345), (7, 0.0, 96.24941183068833, 44.186097183013345), (5, 0.0, 64.16627455379222, 46.906175350571225), (7, 0.0, 32.08313727689611, 48.73422950005006), (5, 0.0, 32.08313727689611, 48.73422950005006), (3, 0.0, 32.08313727689611, 48.73422950005006), \n",
      "\n",
      "(7, 0.0, 32.08313727689611, 46.906175350571225), (3, 0.0, 32.08313727689611, 46.906175350571225), (5, 0.0, 0.0, 1e+100), (3, 0.0, 32.08313727689611, 46.906175350571225), (5, 0.0, 32.08313727689611, 46.906175350571225), (7, 0.0, 0.0, 1e+100), (3, 0.0, 32.08313727689611, 43.85723750205086), (7, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (5, 0.0, 32.08313727689611, 43.85723750205086), (3, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (3, 0.0, 32.08313727689611, 43.85723750205086), (7, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (5, 0.0, 32.08313727689611, 46.906175350571225), (7, 0.0, 32.08313727689611, 46.906175350571225), (3, 0.0, 0.0, 1e+100), (7, 0.0, 32.08313727689611, 46.906175350571225), (5, 0.0, 32.08313727689611, 46.906175350571225), (3, 0.0, 0.0, 1e+100), (7, 0.0, 32.08313727689611, 43.85723750205086), (5, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), \n",
      "\n",
      "(7, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), (7, 0.0, 0.0, 1e+100), (5, 0.0, 0.0, 1e+100), (3, 0.0, 0.0, 1e+100), "
     ]
    }
   ],
   "source": [
    "def print_tree(tree_list=root.children):\n",
    "    #print(tree_list)\n",
    "    if tree_list == []:\n",
    "        return\n",
    "    next_level = []\n",
    "    print()\n",
    "    print()\n",
    "    for tree in tree_list:\n",
    "        try:\n",
    "            print((tree.action, tree.immediate_reward, tree.value_sum, tree.ucb_score()), end=', ')\n",
    "            #print(tree.action, tree.immediate_reward, end=', ')\n",
    "            next_level.extend(tree.children)\n",
    "        except:\n",
    "            pass\n",
    "    print_tree(next_level)\n",
    "    \n",
    "\n",
    "print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n vs 37.9404634616357\n",
      "loss =  tensor(3308.4570, device='cuda:0', grad_fn=<PowBackward0>) tensor(0., device='cuda:0', grad_fn=<SelectBackward>) 57.5191895191067 0\n",
      "n vs 57.5191895191067\n",
      "loss =  tensor(448.1320, device='cuda:0', grad_fn=<PowBackward0>) tensor(6.6438, device='cuda:0', grad_fn=<SelectBackward>) 27.812910553814348 8\n",
      "n vs 27.812910553814348\n",
      "loss =  tensor(1717.6643, device='cuda:0', grad_fn=<PowBackward0>) tensor(7.0588, device='cuda:0', grad_fn=<SelectBackward>) 48.50350291898595 2\n",
      "n vs 48.50350291898595\n",
      "loss =  tensor(572.7789, device='cuda:0', grad_fn=<PowBackward0>) tensor(0., device='cuda:0', grad_fn=<SelectBackward>) 23.932799742943757 0\n",
      "n vs 23.932799742943757\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-df7295415865>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mplan_mcts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-d6224c335282>\u001b[0m in \u001b[0;36mplan_mcts\u001b[0;34m(root, n_iters)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m#self.render()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;31m#print('rol ', value)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-71591c056646>\u001b[0m in \u001b[0;36mrollout\u001b[0;34m(self, t_max, gamma)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_done\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mt_max\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1e-12\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# best action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mrollout_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-71591c056646>\u001b[0m in \u001b[0;36mget_actions\u001b[0;34m(self, obs, agent)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mobs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mq_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_qvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mholy_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m  \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholy_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mholy_3\u001b[0m \u001b[0;31m# for agent train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-4e650bce2a1e>\u001b[0m in \u001b[0;36mget_qvalues\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mlike\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbut\u001b[0m \u001b[0mworks\u001b[0m \u001b[0mon\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mVariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \"\"\"\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mqvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mqvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from itertools import count\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "total_reward = 0                #sum of rewards\n",
    "test_env = loads(root_snapshot) #env used to show progress\n",
    "\n",
    "for i in count():\n",
    "    \n",
    "    #get best child\n",
    "    best_child = max(root.children, key=lambda x: x.ucb_score())\n",
    "    \n",
    "    #take action\n",
    "    s,r,done,_ = test_env.step(best_child.action)\n",
    "    #train_agent(agent, root)\n",
    "    \n",
    "    \n",
    "    #show image\n",
    "    clear_output(True)\n",
    "    plt.title(\"step {}, tr={}\".format(i, (total_reward, best_child.action, r)))\n",
    "    plt.imshow(test_env.render('rgb_array'))\n",
    "    plt.show()\n",
    "    #time.sleep(0.3)\n",
    "    \n",
    "    total_reward += r\n",
    "    if done:\n",
    "        print(\"Finished with reward = \",total_reward)\n",
    "        break\n",
    "    \n",
    "    #discard unrealized part of the tree [because not every child matters :(]\n",
    "    for child in root.children:\n",
    "        if child != best_child:\n",
    "            child.safe_delete()\n",
    "\n",
    "    #declare best child a new root\n",
    "    root = Root.from_node(best_child)\n",
    "    \n",
    "    if root.is_leaf():\n",
    "        plan_mcts(root,n_iters=10)\n",
    "        \n",
    "    \n",
    "    assert not root.is_leaf(), \"We ran out of tree! Need more planning! Try growing tree right inside the loop.\"\n",
    "    \n",
    "    #you may want to expand tree here\n",
    "    #<your code here>\n",
    "    if i == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Planing tree on 1000 iter - just 11 actions in row with hole (why we have hols - becouse rollout give dif score and some child are cuted at source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bonus assignments (10+pts each)\n",
    "\n",
    "There's a few things you might want to try if you want to dig deeper:\n",
    "\n",
    "### Node selection and expansion\n",
    "\n",
    "\"Analyze this\" assignment\n",
    "\n",
    "UCB-1 is a weak bound as it relies on a very general bounds (Hoeffding Inequality, to be exact). \n",
    "* Try playing with alpha. The theoretically optimal alpha for CartPole is 200 (max reward). \n",
    "* Use using a different exploration strategy (bayesian UCB, for example)\n",
    "* Expand not all but several random actions per `expand` call. See __the notes below__ for details.\n",
    "\n",
    "The goal is to find out what gives the optimal performance for `CartPole-v0` for different time budgets (i.e. different n_iter in plan_mcts.\n",
    "\n",
    "Evaluate your results on `AcroBot-v1` - do the results change and if so, how can you explain it?\n",
    "\n",
    "\n",
    "### Atari-RAM\n",
    "\n",
    "\"Build this\" assignment\n",
    "\n",
    "Apply MCTS to play atari games. In particular, let's start with ```gym.make(\"MsPacman-ramDeterministic-v0\")```.\n",
    "\n",
    "This requires two things:\n",
    "* Slightly modify WithSnapshots wrapper to work with atari.\n",
    "\n",
    " * Atari has a special interface for snapshots:\n",
    "   ```   \n",
    "   snapshot = self.env.ale.cloneState()\n",
    "   ...\n",
    "   self.env.ale.restoreState(snapshot)\n",
    "   ```\n",
    " * Try it on the env above to make sure it does what you told it to.\n",
    " \n",
    "* Run MCTS on the game above. \n",
    " * Start with small tree size to speed-up computations\n",
    " * You will probably want to rollout for 10-100 steps (t_max) for starters\n",
    " * Consider using discounted rewards (see __notes at the end__)\n",
    " * Try a better rollout policy\n",
    " \n",
    " \n",
    "### Integrate learning into planning\n",
    "\n",
    "Planning on each iteration is a costly thing to do. You can speed things up drastically if you train a classifier to predict which action will turn out to be best according to MCTS.\n",
    "\n",
    "To do so, just record which action did the MCTS agent take on each step and fit something to [state, mcts_optimal_action]\n",
    "* You can also use optimal actions from discarded states to get more (dirty) samples. Just don't forget to fine-tune without them.\n",
    "* It's also worth a try to use P(best_action|state) from your model to select best nodes in addition to UCB\n",
    "* If your model is lightweight enough, try using it as a rollout policy.\n",
    "\n",
    "__(bonus points)__ While CartPole is glorious enough, try expanding this to ```gym.make(\"MsPacmanDeterministic-v0\")```\n",
    "* See previous section on how to wrap atari (you'll get points for both if you run this on atari)\n",
    "\n",
    "\n",
    "### Integrate planning into learning (project, a LOT of points)\n",
    "\n",
    "Incorporate planning into the agent architecture. \n",
    "\n",
    "The goal is to implement [Value Iteration Networks](https://arxiv.org/abs/1602.02867)\n",
    "\n",
    "For starters, remember [week7 assignment](https://github.com/yandexdataschool/Practical_RL/blob/master/week7/7.2_seminar_kung_fu.ipynb)? If not, use [this](http://bit.ly/2oZ34Ap) instead.\n",
    "\n",
    "You will need to switch it into a maze-like game, consider MsPacman or the games from week7 [Bonus: Neural Maps from here](https://github.com/yandexdataschool/Practical_RL/blob/master/week7/7.3_homework.ipynb).\n",
    "\n",
    "You will need to implement a special layer that performs value iteration-like update to a recurrent memory. This can be implemented the same way you did attention from week7 or week8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(i,v) for i, v  in enumerate(q[0])], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Node = list(root.children)[0]\n",
    "Node.observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(root.children)[0].action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agent(agent, root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what action give reward?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "test_env.reset()\n",
    "act_res = {i: 0 for i in range(9)}\n",
    "for i in range(200):\n",
    "    act = test_env.action_space.sample()\n",
    "    res = test_env.step(act)\n",
    "    act_res[act] += res[1]\n",
    "    plt.imshow(test_env.render('rgb_array'))\n",
    "    plt.show()\n",
    "    #time.sleep(0.3)\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.load_snapshot(snap0)\n",
    "\n",
    "print(\"\\n\\nAfter loading snapshot\")\n",
    "plt.imshow(test_env.render('rgb_array'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_observation = env.reset()\n",
    "root_snapshot = env.get_snapshot()\n",
    "root = Root(root_snapshot,root_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plan_mcts(root, n_iters=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Planing tree on 1000 iter - just 11 actions in row with hole (why we have hols - becouse rollout give dif score and some child are cuted at source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "\n",
    "#### Assumptions\n",
    "\n",
    "The full list of assumptions is\n",
    "* __Finite actions__ - we enumerate all actions in `expand`\n",
    "* __Episodic (finite) MDP__ - while technically it works for infinite mdp, we rollout for $ 10^4$ steps. If you are knowingly infinite, please adjust `t_max` to something more reasonable.\n",
    "* __No discounted rewards__ - we assume $\\gamma=1$. If that isn't the case, you only need to change a two lines in `rollout` and use `my_R = r + gamma*child_R` for `propagate`\n",
    "* __pickleable env__ - won't work if e.g. your env is connected to a web-browser surfing the internet. For custom envs, you may need to modify get_snapshot/load_snapshot from `WithSnapshots`.\n",
    "\n",
    "#### On `get_best_leaf` and `expand` functions\n",
    "\n",
    "This MCTS implementation only selects leaf nodes for expansion.\n",
    "This doesn't break things down because `expand` adds all possible actions. Hence, all non-leaf nodes are by design fully expanded and shouldn't be selected.\n",
    "\n",
    "If you want to only add a few random action on each expand, you will also have to modify `get_best_leaf` to consider returning non-leafs.\n",
    "\n",
    "#### Rollout policy\n",
    "\n",
    "We use a simple uniform policy for rollouts. This introduces a negative bias to good situations that can be messed up completely with random bad action. As a simple example, if you tend to rollout with uniform policy, you better don't use sharp knives and walk near cliffs.\n",
    "\n",
    "You can improve that by integrating a reinforcement _learning_ algorithm with a computationally light agent. You can even train this agent on optimal policy found by the tree search.\n",
    "\n",
    "#### Contributions\n",
    "* Reusing some code from 5vision [solution for deephack.RL](https://github.com/5vision/uct_atari), code by Mikhail Pavlov\n",
    "* Using some code from [this gist](https://gist.github.com/blole/dfebbec182e6b72ec16b66cc7e331110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tourch_gym]",
   "language": "python",
   "name": "conda-env-tourch_gym-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
